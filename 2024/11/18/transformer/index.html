<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/huh.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/huh.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="世界名画">
<meta property="og:type" content="article">
<meta property="og:title" content="我对Transformer的理解">
<meta property="og:url" content="http://example.com/2024/11/18/transformer/index.html">
<meta property="og:site_name" content="PIGMilk的博客">
<meta property="og:description" content="世界名画">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/transformer/1.png">
<meta property="og:image" content="http://example.com/images/transformer/2.png">
<meta property="og:image" content="http://example.com/images/transformer/3.png">
<meta property="og:image" content="http://example.com/images/transformer/4.png">
<meta property="og:image" content="http://example.com/images/transformer/5.jpg">
<meta property="og:image" content="http://example.com/images/transformer/6.png">
<meta property="article:published_time" content="2024-11-17T16:19:05.000Z">
<meta property="article:modified_time" content="2024-11-30T00:59:16.314Z">
<meta property="article:author" content="PIGMilk">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/transformer/1.png">


<link rel="canonical" href="http://example.com/2024/11/18/transformer/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/11/18/transformer/","path":"2024/11/18/transformer/","title":"我对Transformer的理解"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>我对Transformer的理解 | PIGMilk的博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">PIGMilk的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">PIGMIlk's Blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-主页"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a></li><li class="menu-item menu-item-个人简介"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>个人简介</a></li><li class="menu-item menu-item-分类"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>分类</a></li><li class="menu-item menu-item-归档"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%96%E7%95%8C%E5%90%8D%E7%94%BB"><span class="nav-number">1.</span> <span class="nav-text">世界名画</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Input-Embedding"><span class="nav-number">2.</span> <span class="nav-text">Input Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Positional-Encoding"><span class="nav-number">3.</span> <span class="nav-text">Positional Encoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%9B%E5%85%A5Encoder"><span class="nav-number"></span> <span class="nav-text">进入Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%AD%90%E5%B1%82%E2%80%94%E2%80%94Multi-Head-Attention"><span class="nav-number">1.</span> <span class="nav-text">第一个子层——Multi-Head Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2"><span class="nav-number">1.1.</span> <span class="nav-text">线性变换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9D%83%E9%87%8D"><span class="nav-number">1.2.</span> <span class="nav-text">计算注意力权重</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E5%A4%84%E7%90%86"><span class="nav-number">1.3.</span> <span class="nav-text">多头处理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Add-Norm"><span class="nav-number">2.</span> <span class="nav-text">Add&amp;Norm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Add-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5"><span class="nav-number">2.1.</span> <span class="nav-text">Add(残差连接)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Norm-%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96LayerNorm"><span class="nav-number">2.2.</span> <span class="nav-text">Norm(层归一化LayerNorm)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feed-Forward"><span class="nav-number">3.</span> <span class="nav-text">Feed Forward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%88%E6%98%AF%E4%B8%80%E4%B8%AAAdd-Norm"><span class="nav-number">4.</span> <span class="nav-text">又是一个Add&amp;Norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Output-Embedding"><span class="nav-number">5.</span> <span class="nav-text">Output Embedding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%9B%E5%85%A5Decoder"><span class="nav-number"></span> <span class="nav-text">进入Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Masked-Multi-Head-Attention"><span class="nav-number">1.</span> <span class="nav-text">Masked Multi-Head Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Mask%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86"><span class="nav-number">1.1.</span> <span class="nav-text">Mask的实现原理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%88%E5%8F%88%E6%98%AF%E4%B8%80%E4%B8%AAAdd-Norm"><span class="nav-number">2.</span> <span class="nav-text">又又是一个Add&amp;Norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E4%B8%80%E4%B8%AAMulti-Head-Attention"><span class="nav-number">3.</span> <span class="nav-text">下一个Multi-Head Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%92%E6%98%AF%E4%B8%80%E4%B8%AAAdd-Norm"><span class="nav-number">4.</span> <span class="nav-text">叒是一个Add&amp;Norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%84%B6%E5%90%8E%E6%98%AF%E5%8F%88%E6%98%AFFeed-Forward"><span class="nav-number">5.</span> <span class="nav-text">然后是又是Feed-Forward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%95%E6%98%AF%E4%B8%80%E4%B8%AAAdd-Norm"><span class="nav-number">6.</span> <span class="nav-text">叕是一个Add&amp;Norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear"><span class="nav-number">7.</span> <span class="nav-text">Linear</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E7%9A%84%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E7%9F%A9%E9%98%B5%E4%B8%AD%EF%BC%8C%E5%8F%AA%E6%9C%89%E6%9C%80%E5%90%8E%E4%B8%80%E8%A1%8C%E6%98%AF%E6%9C%89%E7%94%A8%E7%9A%84-%E5%AF%B9%E5%BA%94%E7%9A%84%E6%98%AF%E9%A2%84%E6%B5%8B%E7%9A%84%E6%96%B0%E4%B8%80%E4%B8%AA%E8%AF%8D-%E4%BD%86%E6%98%AF%E9%9C%80%E8%A6%81%E8%BE%93%E5%87%BA%E6%95%B4%E4%B8%AA%E7%9F%A9%E9%98%B5%EF%BC%8C%E5%9B%A0%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E9%9C%80%E8%A6%81%E6%AF%94%E8%BE%83%E6%AF%8F%E4%B8%AA%E4%BD%8D%E7%BD%AE%E7%9A%84%E9%A2%84%E6%B5%8B%E5%80%BC%E5%92%8C%E7%9C%9F%E5%AE%9E%E5%80%BC%E3%80%82"><span class="nav-number"></span> <span class="nav-text">输出的概率分布矩阵中，只有最后一行是有用的(对应的是预测的新一个词)但是需要输出整个矩阵，因为损失函数需要比较每个位置的预测值和真实值。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%A7%A3"><span class="nav-number"></span> <span class="nav-text">理解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA"><span class="nav-number">1.</span> <span class="nav-text">输入和输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K%E5%92%8CV%E6%9D%A5%E8%87%AAEncoder%EF%BC%8CQ%E6%9D%A5%E8%87%AADecoder"><span class="nav-number">2.</span> <span class="nav-text">K和V来自Encoder，Q来自Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8LayerNorm%EF%BC%9F"><span class="nav-number">3.</span> <span class="nav-text">为什么用LayerNorm？</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="PIGMilk"
      src="/images/turtle.png">
  <p class="site-author-name" itemprop="name">PIGMilk</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Keiran810975" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Keiran810975" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:23676866@qq.com" title="E-Mail → mailto:23676866@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/yourname" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/yourname" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/yourname" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-facebook fa-fw"></i>FB Page</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/yourname" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://youtube.com/yourname" title="YouTube → https:&#x2F;&#x2F;youtube.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/yourname" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
      <span class="links-of-author-item">
        <a href="skype:yourname?call|chat" title="Skype → skype:yourname?call|chat" rel="noopener me" target="_blank"><i class="fab fa-skype fa-fw"></i>Skype</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/big/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/18/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/turtle.png">
      <meta itemprop="name" content="PIGMilk">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PIGMilk的博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="我对Transformer的理解 | PIGMilk的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          我对Transformer的理解
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-11-18 00:19:05" itemprop="dateCreated datePublished" datetime="2024-11-18T00:19:05+08:00">2024-11-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-11-30 08:59:16" itemprop="dateModified" datetime="2024-11-30T08:59:16+08:00">2024-11-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h3 id="世界名画"><a href="#世界名画" class="headerlink" title="世界名画"></a>世界名画</h3><p><img src="/../images/transformer/1.png" alt="1"></p>
<span id="more"></span>


<h3 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input Embedding"></a>Input Embedding</h3><p>原始输入一般是离散的符号(单词)，通过一个词汇表&#x2F;字典，映射成索引表示。<br>然后每个离散值的索引被转换成一个固定长度的向量，比如<code>1-&gt;[0.25, 0.13, 0.88, ...]</code>，这个向量的长度由预设好的参数d_model决定。</p>
<p>这个转换是可学习的，通常是一个矩阵W(<strong>嵌入矩阵</strong>)，大小是<strong>V*d_model</strong>，V是词汇表的大小。通过输入索引来查找对应行，得到符号对应的<strong>嵌入向量</strong>。</p>
<ul>
<li>嵌入矩阵是可学习的，所以符号可以通过嵌入矩阵的不断学习得到更准确的表示，。</li>
<li>语义，用法相近，关联较大的单词在空间中距离较近</li>
</ul>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>Transformer中，位置编码使用如下公式:<br><img src="/../images/transformer/2.png" alt="2"></p>
<ul>
<li>pos: 当前符号的位置（序列的索引）</li>
<li>i: 嵌入向量的维度索引（从0到d_model-1）</li>
</ul>
<p>这样一个嵌入向量就会对应到一个位置向量<br>所有位置向量得到<strong>位置编码矩阵PE</strong>(L*d_model)<br>其中L是序列的长度</p>
<p>然后嵌入矩阵和位置编码矩阵<strong>逐元素相加</strong>,得到矩阵<strong>H</strong>(L*d_model)，作为输入。</p>
<hr>
<h2 id="进入Encoder"><a href="#进入Encoder" class="headerlink" title="进入Encoder"></a>进入Encoder</h2><h3 id="第一个子层——Multi-Head-Attention"><a href="#第一个子层——Multi-Head-Attention" class="headerlink" title="第一个子层——Multi-Head Attention"></a>第一个子层——Multi-Head Attention</h3><h4 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h4><p>把矩阵H投影到三个向量空间，即通过三个矩阵，分别生成查询Q，键Key，值Value<br><code>Q=H·Wq</code>  <code>K=H·Wk</code>  <code>V=H·Wv</code><br>其中Wq，Wk，Wv维度是d_model<em>dk，是可学习的矩阵<br>Q，K，V维度是L</em>dk</p>
<h4 id="计算注意力权重"><a href="#计算注意力权重" class="headerlink" title="计算注意力权重"></a>计算注意力权重</h4><p>通过如下公式计算注意力分数：<br><img src="/../images/transformer/3.png" alt="3"></p>
<ul>
<li>QK^T^(L*L)：查询和键的点积，衡量序列中每两个符号之间的相关性大小</li>
<li>1&#x2F;dk^1&#x2F;2^：缩放，防止梯度爆炸或消失</li>
<li>使用softmax归一化，使注意力分数成为概率分布</li>
<li>再乘上值矩阵V，得到Attention(Q,K,V) (L*dk)，就是序列的加权表示</li>
</ul>
<h4 id="多头处理"><a href="#多头处理" class="headerlink" title="多头处理"></a>多头处理</h4><p>为了让模型能够关注不同的上下文信息，使用多个头来计算注意力，每个头独立进行操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">headi=Attention(Qi,Ki,Vi)</span><br></pre></td></tr></table></figure>

<p>将所有头的结果拼接后，再通过一个线性层进行变换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MultiHead(Q,K,V)=Concat(head1,head2,……,headh)·Wo</span><br></pre></td></tr></table></figure>

<ul>
<li>Wo是输出线性变换的权重，Wo可学习</li>
<li>每个注意力头都会有一个Wq，Wk，Wv</li>
<li>Concat的意思是把所有得到的注意力头<strong>按列拼接成一个大矩阵</strong>，大小是*<em>L</em>(h·dk)**，其中h是注意力头的数量</li>
<li>Wo大小是(h·dk)*d_model</li>
<li>MutiHead: L*d_model</li>
</ul>
<h3 id="Add-Norm"><a href="#Add-Norm" class="headerlink" title="Add&amp;Norm"></a>Add&amp;Norm</h3><h4 id="Add-残差连接"><a href="#Add-残差连接" class="headerlink" title="Add(残差连接)"></a>Add(残差连接)</h4><p>直接把Input加到多头注意力的输出上，得到Add的Output。<br>这样可以<strong>保留输入的原始特征</strong>，即使多头注意力的结果不够理想，模型仍可以保留原始特征，避免信息丢失</p>
<h4 id="Norm-层归一化LayerNorm"><a href="#Norm-层归一化LayerNorm" class="headerlink" title="Norm(层归一化LayerNorm)"></a>Norm(层归一化LayerNorm)</h4><p>再对AddOutput进行归一化，公式如下：<br><img src="/../images/transformer/4.png" alt="4"></p>
<ul>
<li>归一化可以缓解梯度爆炸或梯度消失问题</li>
<li>归一化能够让模型更快地达到收敛状态</li>
</ul>
<p><strong>关于LayerNorm</strong><br>一般的归一化是以batch为单位，对每个feature里的所有样本进行归一化<br>而LayerNorm是以特征的维度为单位，<strong>对每个样本的所有特征进行归一化</strong></p>
<p>这样做的优势是在序列长度或批次大小变化时，归一化效果不受影响，序列中的每个时间步可以独立归一化，不依赖于其他时间步或样本，比较适合NLP。</p>
<h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h3><p>一个隐藏层的前馈神经网络</p>
<h3 id="又是一个Add-Norm"><a href="#又是一个Add-Norm" class="headerlink" title="又是一个Add&amp;Norm"></a>又是一个Add&amp;Norm</h3><p>和之前那个Add&amp;Norm一样</p>
<p>最后输出是L*d_model</p>
<h3 id="Output-Embedding"><a href="#Output-Embedding" class="headerlink" title="Output Embedding"></a>Output Embedding</h3><p>解码器输入的是目标序列(就是正确答案)<br>编码器和解码器的嵌入矩阵可以是同一个(共享)，也可以是不同的，取决于具体要解决的问题</p>
<p>这里的输入和Encoder的输入做同样的处理，把得到的嵌入矩阵和位置编码矩阵相加，得到矩阵H，作为Decoder的输入。</p>
<h2 id="进入Decoder"><a href="#进入Decoder" class="headerlink" title="进入Decoder"></a>进入Decoder</h2><h3 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h3><p>因为模型训练的过程本质上是让模型先做预测，然后再反向传播调整参数，所以在训练中decoder做预测，是不能让它看见正确答案的。这样就需要引入Mask，输入到句子的某一处，decoder只能看到这里，而看不到后面(被掩盖)。</p>
<p>这个多头注意力的Encoder中的相同。</p>
<h4 id="Mask的实现原理"><a href="#Mask的实现原理" class="headerlink" title="Mask的实现原理"></a>Mask的实现原理</h4><p>Mask 是一个上三角矩阵，<strong>主对角线以下的元素为 0，主对角线以上的元素为负无穷</strong>。<br>把Mask加到注意力的分的计算中，如图：<br><img src="/../images/transformer/5.jpg" alt="5"></p>
<p>这样处理之后，Softmax就会把<strong>被mask的位置变成0</strong>，就相当于权重是0，这样就实现了屏蔽未来的词对现在预测的影响</p>
<p>贴一下transformer里的Softmax，防止忘了：<br><img src="/../images/transformer/6.png" alt="6"></p>
<h3 id="又又是一个Add-Norm"><a href="#又又是一个Add-Norm" class="headerlink" title="又又是一个Add&amp;Norm"></a>又又是一个Add&amp;Norm</h3><h3 id="下一个Multi-Head-Attention"><a href="#下一个Multi-Head-Attention" class="headerlink" title="下一个Multi-Head Attention"></a>下一个Multi-Head Attention</h3><p>这个多头注意力层，查询Q来自上一层Add&amp;Norm的输出乘Wq<br>而K和V来自Encoder的输出乘Wk，Wv</p>
<h3 id="叒是一个Add-Norm"><a href="#叒是一个Add-Norm" class="headerlink" title="叒是一个Add&amp;Norm"></a>叒是一个Add&amp;Norm</h3><h3 id="然后是又是Feed-Forward"><a href="#然后是又是Feed-Forward" class="headerlink" title="然后是又是Feed-Forward"></a>然后是又是Feed-Forward</h3><p>和Encoder中的相似，就不说了</p>
<h3 id="叕是一个Add-Norm"><a href="#叕是一个Add-Norm" class="headerlink" title="叕是一个Add&amp;Norm"></a>叕是一个Add&amp;Norm</h3><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><p>Decoder中的最后一层Add&amp;Norm之后，会输出一个矩阵H(n<em>d_model)<br>其中n是目标序列的长度<br>然后做一个线性变换，乘一个权重矩阵W(d_model</em>V)，再加上偏置b<br>最终输出矩阵Z(n*V),V是目标词汇表的大小<br><strong>Linear 层将 Decoder 输出的高维特征(d_model)投影到目标词汇表大小 V的维度空间，方便后续 Softmax 转化为概率分布。</strong><br>每个时间步 t 的向量 Z t表示目标词汇表中每个单词的未归一化分数（logits）</p>
<p>然后用Softmax把logits转为概率分布，对每个时间步t，选择概率最大的词的索引，得到一个索引序列，就是目标序列的预测结果</p>
<p>然后计算<strong>交叉熵损失</strong>，回去训练，最小化交叉熵损失</p>
<h2 id="输出的概率分布矩阵中，只有最后一行是有用的-对应的是预测的新一个词-但是需要输出整个矩阵，因为损失函数需要比较每个位置的预测值和真实值。"><a href="#输出的概率分布矩阵中，只有最后一行是有用的-对应的是预测的新一个词-但是需要输出整个矩阵，因为损失函数需要比较每个位置的预测值和真实值。" class="headerlink" title="输出的概率分布矩阵中，只有最后一行是有用的(对应的是预测的新一个词)但是需要输出整个矩阵，因为损失函数需要比较每个位置的预测值和真实值。"></a>输出的概率分布矩阵中，只有最后一行是有用的(对应的是预测的新一个词)<br>但是需要输出整个矩阵，因为损失函数需要比较每个位置的预测值和真实值。</h2><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>上面大致地理清了transformer的结构，现在我们来细致地谈谈对它的理解</p>
<h3 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h3><p>Transformer最开始设计出来，主要是用于<strong>机器翻译</strong>工作。<br>Encoder的原始输入是源语言的句子，比如英语，Decoder的原始输入则是对应的目标语言的翻译句子，比如德语<br>Encoder的输出是一个L<em>dk的矩阵，L是输入序列的token数量，每一行是一个token的d_model维的向量，是源序列中对应词的向量，它含有这个词在上下文里的语义信息。Decoder输出的是一个n</em>V的矩阵，n是目标序列的token数量，每一行的大小都是词汇表的大小，然后做一个softmax，就可以得到每个词的概率分布，然后选择概率最大的词作为翻译结果。</p>
<h3 id="K和V来自Encoder，Q来自Decoder"><a href="#K和V来自Encoder，Q来自Decoder" class="headerlink" title="K和V来自Encoder，Q来自Decoder"></a>K和V来自Encoder，Q来自Decoder</h3><p>K，V提供源序列的上下文表示，包含了源序列的全局语义信息和词间关系<br>Q来自Decoder的目标序列的隐藏状态，表示当前生成过程中需要的上下文信息。<br>这样就能实现输出的序列会学习源序列的语义信息</p>
<p><strong>注意两种语言用的不是同一个词汇表。</strong></p>
<h3 id="为什么用LayerNorm？"><a href="#为什么用LayerNorm？" class="headerlink" title="为什么用LayerNorm？"></a>为什么用LayerNorm？</h3><p>在Transformer中，输入通常是序列数据（句子），不同批次间的序列长度可能不一致，使用 BatchNorm 可能导致计算均值和方差的不稳定<br>而LayerNorm 对每个样本单独归一化，<strong>不依赖批量大小</strong>，因此适合可变长序列的建模<br>Transformer中容易出现梯度不稳定或过大的激活值，LayerNorm 可以有效平滑激活值，确保梯度的传播稳定</p>
<p>(以后想到再补充)</p>
<p><strong>Copy is all you need</strong></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/11/17/banzi/" rel="prev" title="(大合集)算法板子+经典题代码">
                  <i class="fa fa-angle-left"></i> (大合集)算法板子+经典题代码
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/11/21/cnn/" rel="next" title="关于CNN">
                  关于CNN <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">PIGMilk</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"http://example.com/2024/11/18/transformer/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
