<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/huh.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/huh.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="ğŸ¥• ğŸŒ½ ğŸŒ¶ï¸ ğŸ«‘ ğŸ¥’ ğŸ¥¬ ğŸ¥¦ğŸŒ­ ğŸ” ğŸŸ ğŸ• ğŸ¥ª ğŸŒ® ğŸ¥™">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorchç¬”è®°">
<meta property="og:url" content="http://example.com/2024/12/01/pytorch2/index.html">
<meta property="og:site_name" content="PIGMilkçš„åšå®¢">
<meta property="og:description" content="ğŸ¥• ğŸŒ½ ğŸŒ¶ï¸ ğŸ«‘ ğŸ¥’ ğŸ¥¬ ğŸ¥¦ğŸŒ­ ğŸ” ğŸŸ ğŸ• ğŸ¥ª ğŸŒ® ğŸ¥™">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-12-01T09:55:06.000Z">
<meta property="article:modified_time" content="2024-12-03T02:03:05.811Z">
<meta property="article:author" content="PIGMilk">
<meta property="article:tag" content="æ·±åº¦å­¦ä¹ ">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2024/12/01/pytorch2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/12/01/pytorch2/","path":"2024/12/01/pytorch2/","title":"PyTorchç¬”è®°"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>PyTorchç¬”è®° | PIGMilkçš„åšå®¢</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">PIGMilkçš„åšå®¢</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">PIGMIlk's Blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-ä¸»é¡µ"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>ä¸»é¡µ</a></li><li class="menu-item menu-item-ä¸ªäººç®€ä»‹"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>ä¸ªäººç®€ä»‹</a></li><li class="menu-item menu-item-åˆ†ç±»"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>åˆ†ç±»</a></li><li class="menu-item menu-item-å½’æ¡£"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Conda%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="nav-number">1.</span> <span class="nav-text">Condaè™šæ‹Ÿç¯å¢ƒçš„åˆ›å»º</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pytorch%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">PytorchåŸºæœ¬ä½¿ç”¨</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor"><span class="nav-number">2.1.</span> <span class="nav-text">Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-number">2.2.</span> <span class="nav-text">è‡ªåŠ¨æ±‚å¯¼</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nn-Module"><span class="nav-number">2.3.</span> <span class="nav-text">nn.Module</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8-Optimizer"><span class="nav-number">2.4.</span> <span class="nav-text">ä¼˜åŒ–å™¨(Optimizer)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SGD"><span class="nav-number">2.4.1.</span> <span class="nav-text">SGD</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF%EF%BC%88Training-Loop%EF%BC%89"><span class="nav-number">2.5.</span> <span class="nav-text">è®­ç»ƒå¾ªç¯ï¼ˆTraining Loopï¼‰</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam"><span class="nav-number">2.5.1.</span> <span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD-DataLoader"><span class="nav-number">2.6.</span> <span class="nav-text">æ•°æ®åŠ è½½(DataLoader)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E9%AA%8C%E8%AF%81"><span class="nav-number">2.7.</span> <span class="nav-text">è®­ç»ƒå’ŒéªŒè¯</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">2.7.1.</span> <span class="nav-text">è®­ç»ƒ</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81"><span class="nav-number">2.7.2.</span> <span class="nav-text">éªŒè¯</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="nav-number">2.8.</span> <span class="nav-text">æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-nn"><span class="nav-number">3.</span> <span class="nav-text">torch.nn</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">4.</span> <span class="nav-text">ç¤ºä¾‹</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%B8%AAMLP%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.1.</span> <span class="nav-text">ä¸€ä¸ªMLPçš„å®ç°</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ViT%E6%A8%A1%E5%9E%8B-%E8%AE%AD%E7%BB%83"><span class="nav-number">4.2.</span> <span class="nav-text">ViTæ¨¡å‹+è®­ç»ƒ</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%83%A8%E5%88%86"><span class="nav-number">4.2.1.</span> <span class="nav-text">æ¨¡å‹éƒ¨åˆ†</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E9%83%A8%E5%88%86"><span class="nav-number">4.2.2.</span> <span class="nav-text">è®­ç»ƒéƒ¨åˆ†</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="PIGMilk"
      src="/images/turtle.png">
  <p class="site-author-name" itemprop="name">PIGMilk</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Keiran810975" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;Keiran810975" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:23676866@qq.com" title="E-Mail â†’ mailto:23676866@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/yourname" title="Weibo â†’ https:&#x2F;&#x2F;weibo.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/yourname" title="Twitter â†’ https:&#x2F;&#x2F;twitter.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/yourname" title="FB Page â†’ https:&#x2F;&#x2F;www.facebook.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-facebook fa-fw"></i>FB Page</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/yourname" title="StackOverflow â†’ https:&#x2F;&#x2F;stackoverflow.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://youtube.com/yourname" title="YouTube â†’ https:&#x2F;&#x2F;youtube.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/yourname" title="Instagram â†’ https:&#x2F;&#x2F;instagram.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
      <span class="links-of-author-item">
        <a href="skype:yourname?call|chat" title="Skype â†’ skype:yourname?call|chat" rel="noopener me" target="_blank"><i class="fab fa-skype fa-fw"></i>Skype</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/big/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/01/pytorch2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/turtle.png">
      <meta itemprop="name" content="PIGMilk">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PIGMilkçš„åšå®¢">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="PyTorchç¬”è®° | PIGMilkçš„åšå®¢">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorchç¬”è®°
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-01 17:55:06" itemprop="dateCreated datePublished" datetime="2024-12-01T17:55:06+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-12-03 10:03:05" itemprop="dateModified" datetime="2024-12-03T10:03:05+08:00">2024-12-03</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>ğŸ¥• ğŸŒ½ ğŸŒ¶ï¸ ğŸ«‘ ğŸ¥’ ğŸ¥¬ ğŸ¥¦<br>ğŸŒ­ ğŸ” ğŸŸ ğŸ• ğŸ¥ª ğŸŒ® ğŸ¥™</p>
<span id="more"></span>

<h2 id="Condaè™šæ‹Ÿç¯å¢ƒçš„åˆ›å»º"><a href="#Condaè™šæ‹Ÿç¯å¢ƒçš„åˆ›å»º" class="headerlink" title="Condaè™šæ‹Ÿç¯å¢ƒçš„åˆ›å»º"></a>Condaè™šæ‹Ÿç¯å¢ƒçš„åˆ›å»º</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ conda create -n name   # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ</span><br><span class="line">$ conda info -e          # æŸ¥çœ‹è™šæ‹Ÿç¯å¢ƒ</span><br><span class="line">$ conda activate name    # æ¿€æ´»</span><br><span class="line">$ conda deactivate name  # é€€å‡º</span><br></pre></td></tr></table></figure>
<p>å¦‚æœç»ˆç«¯çš„é»˜è®¤ç¯å¢ƒå·²ç»æ˜¯è™šæ‹Ÿç¯å¢ƒäº†ï¼Œé‚£å°±ä¸ç”¨åœ¨condaé‡Œæ¿€æ´»</p>
<h2 id="PytorchåŸºæœ¬ä½¿ç”¨"><a href="#PytorchåŸºæœ¬ä½¿ç”¨" class="headerlink" title="PytorchåŸºæœ¬ä½¿ç”¨"></a>PytorchåŸºæœ¬ä½¿ç”¨</h2><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p>tensorç±»ä¼¼äº NumPy æ•°ç»„ï¼Œä½†å®ƒæ”¯æŒ GPU åŠ é€Ÿ</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])<span class="comment"># ä»åˆ—è¡¨åˆ›å»ºä¸€ä¸ªå¼ é‡</span></span><br><span class="line">zeros = torch.zeros(<span class="number">3</span>, <span class="number">3</span>) <span class="comment"># åˆ›å»ºä¸€ä¸ª 3x3 çš„é›¶çŸ©é˜µ</span></span><br><span class="line">ones = torch.ones(<span class="number">3</span>, <span class="number">3</span>)<span class="comment"># åˆ›å»ºä¸€ä¸ª 3x3 çš„å•ä½çŸ©é˜µ</span></span><br><span class="line">rand = torch.rand(<span class="number">3</span>, <span class="number">3</span>)<span class="comment"># åˆ›å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„çŸ©é˜µ</span></span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">d = torch.matmul(a, b)<span class="comment"># çŸ©é˜µä¹˜æ³•</span></span><br></pre></td></tr></table></figure>
<p>è½¬æ¢è®¾å¤‡ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">a = a.to(device)</span><br></pre></td></tr></table></figure>
<p>è¿ç®—ï¼š</p>
<table>
<thead>
<tr>
<th><strong>å‡½æ•°</strong></th>
<th><strong>åŠŸèƒ½</strong></th>
</tr>
</thead>
<tbody><tr>
<td><code>torch.abs(A)</code></td>
<td>ç»å¯¹å€¼</td>
</tr>
<tr>
<td><code>torch.add(A, B)</code></td>
<td>ç›¸åŠ ï¼ŒA å’Œ B æ—¢å¯ä»¥æ˜¯ Tensor ä¹Ÿå¯ä»¥æ˜¯æ ‡é‡</td>
</tr>
<tr>
<td><code>torch.clamp(A, max, min)</code></td>
<td>è£å‰ªï¼ŒA ä¸­çš„æ•°æ®è‹¥å°äº <code>min</code> æˆ–å¤§äº <code>max</code>ï¼Œåˆ™å˜æˆ <code>min</code> æˆ– <code>max</code>ï¼Œå³ä¿è¯èŒƒå›´åœ¨ <code>[min, max]</code></td>
</tr>
<tr>
<td><code>torch.div(A, B)</code></td>
<td>ç›¸é™¤ï¼ŒA % Bï¼ŒA å’Œ B æ—¢å¯ä»¥æ˜¯ Tensor ä¹Ÿå¯ä»¥æ˜¯æ ‡é‡</td>
</tr>
<tr>
<td><code>torch.mul(A, B)</code></td>
<td>ç‚¹ä¹˜ï¼ŒA * Bï¼ŒA å’Œ B æ—¢å¯ä»¥æ˜¯ Tensor ä¹Ÿå¯ä»¥æ˜¯æ ‡é‡</td>
</tr>
<tr>
<td><code>torch.pow(A, n)</code></td>
<td>æ±‚å¹‚ï¼ŒA çš„ n æ¬¡æ–¹</td>
</tr>
<tr>
<td><code>torch.mm(A, B.T)</code></td>
<td>çŸ©é˜µå‰ä¹˜ï¼Œæ³¨æ„ä¸ <code>torch.mul</code> ä¹‹é—´çš„åŒºåˆ«</td>
</tr>
<tr>
<td><code>torch.mv(A, B)</code></td>
<td>çŸ©é˜µä¸å‘é‡ç›¸ä¹˜ï¼ŒA æ˜¯çŸ©é˜µï¼ŒB æ˜¯å‘é‡ï¼Œè¿™é‡Œçš„ B æ˜¯å¦éœ€è¦è½¬ç½®éƒ½å¯ä»¥</td>
</tr>
<tr>
<td><code>A.item()</code></td>
<td>å°† Tensor è½¬åŒ–ä¸ºåŸºæœ¬æ•°æ®ç±»å‹ï¼Œæ³¨æ„ Tensor ä¸­åªæœ‰ä¸€ä¸ªå…ƒç´ æ—¶æ‰å¯ä»¥ä½¿ç”¨ï¼Œå¸¸ç”¨äºåœ¨ Tensor ä¸­å–å‡ºæ•°å€¼</td>
</tr>
<tr>
<td><code>A.numpy()</code></td>
<td>å°† Tensor è½¬åŒ–ä¸º Numpy ç±»å‹</td>
</tr>
<tr>
<td><code>A.size()</code></td>
<td>æŸ¥çœ‹å°ºå¯¸</td>
</tr>
<tr>
<td><code>A.shape</code></td>
<td>æŸ¥çœ‹å°ºå¯¸ï¼ˆç­‰åŒäº <code>A.size()</code>ï¼‰</td>
</tr>
<tr>
<td><code>A.dtype</code></td>
<td>æŸ¥çœ‹æ•°æ®ç±»å‹</td>
</tr>
<tr>
<td><code>A.view()</code></td>
<td>é‡æ„å¼ é‡å°ºå¯¸ï¼Œç±»ä¼¼äº Numpy ä¸­çš„ <code>reshape</code></td>
</tr>
<tr>
<td><code>A.transpose(0, 1)</code></td>
<td>è¡Œåˆ—äº¤æ¢</td>
</tr>
<tr>
<td><code>A[1:]</code></td>
<td>åˆ‡ç‰‡æ“ä½œ</td>
</tr>
<tr>
<td><code>A[-1, -1] = 100</code></td>
<td>èµ‹å€¼æ“ä½œï¼Œç±»ä¼¼äº Numpy ä¸­çš„åˆ‡ç‰‡</td>
</tr>
<tr>
<td><code>A.zero_()</code></td>
<td>å½’é›¶åŒ–</td>
</tr>
<tr>
<td><code>torch.stack((A, B), dim=-1)</code></td>
<td>æ‹¼æ¥ï¼Œå‡ç»´</td>
</tr>
<tr>
<td><code>torch.diag(A)</code></td>
<td>å– A å¯¹è§’çº¿å…ƒç´ å½¢æˆä¸€ä¸ªä¸€ç»´å‘é‡</td>
</tr>
<tr>
<td><code>torch.diag_embed(A)</code></td>
<td>å°†ä¸€ç»´å‘é‡æ”¾åˆ°å¯¹è§’çº¿ä¸­ï¼Œå…¶ä½™æ•°å€¼ä¸º 0 çš„ Tensor</td>
</tr>
</tbody></table>
<h3 id="è‡ªåŠ¨æ±‚å¯¼"><a href="#è‡ªåŠ¨æ±‚å¯¼" class="headerlink" title="è‡ªåŠ¨æ±‚å¯¼"></a>è‡ªåŠ¨æ±‚å¯¼</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åˆ›å»ºå¼ é‡æ—¶è®¾ç½® requires_grad=True è¡¨ç¤ºéœ€è¦è®¡ç®—æ¢¯åº¦</span></span><br><span class="line"><span class="comment"># å¯¹æ ‡é‡è‡ªåŠ¨æ±‚å¯¼</span></span><br><span class="line">x = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">3</span></span><br><span class="line">y.backward()  <span class="comment"># è®¡ç®— dy/dx</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 12.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># å¯¹éæ ‡é‡æ±‚å¯¼ï¼ˆæä¾›è‡ªå®šä¹‰æ¢¯åº¦ï¼‰</span></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">gradient = torch.tensor([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>])  <span class="comment"># è‡ªå®šä¹‰æ¢¯åº¦</span></span><br><span class="line">y.backward(gradient=gradient)</span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 2x = [2.0, 4.0, 6.0]</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>åœ¨æ¨ç†é˜¶æ®µï¼ˆå¦‚æµ‹è¯•æˆ–é¢„æµ‹ï¼‰é€šå¸¸ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œå¯ä»¥é€šè¿‡ torch.no_grad() ä¸´æ—¶ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å’ŒåŠ é€Ÿè®¡ç®—ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = x ** <span class="number">2</span>  <span class="comment"># ä¸ä¼šè®°å½•è®¡ç®—å›¾</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>å¦‚æœåªæƒ³ä½¿ç”¨å¼ é‡çš„å€¼è€Œä¸éœ€è¦æ±‚å¯¼ï¼Œå¯ä»¥é€šè¿‡ detach() æ–¹æ³•è·å¾—ä¸€ä¸ªä¸éœ€è¦æ¢¯åº¦çš„å‰¯æœ¬:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = x.detach()</span><br></pre></td></tr></table></figure>
</li>
<li><p>æ¯æ¬¡è°ƒç”¨ backward() ä¹‹å‰ï¼Œåº”æ‰‹åŠ¨æ¸…é™¤æ¢¯åº¦ï¼Œé¿å…ç´¯ç§¯ï¼ˆå› ä¸º PyTorch é»˜è®¤ä¼šç´¯ç§¯æ¢¯åº¦ï¼‰</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h3><p>ç¥ç»ç½‘ç»œæ¨¡å‹æ˜¯é€šè¿‡ç»§æ‰¿ nn.Module ç±»æ¥å®šä¹‰çš„ã€‚forward æ–¹æ³•ç”¨äºå®šä¹‰ç½‘ç»œçš„å‰å‘ä¼ æ’­ã€‚ </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># è¾“å…¥å±‚åˆ°éšè—å±‚</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># éšè—å±‚åˆ°è¾“å‡ºå±‚</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))  <span class="comment"># ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»ºæ¨¡å‹å®ä¾‹</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ‰“å°æ¨¡å‹ç»“æ„</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<h3 id="ä¼˜åŒ–å™¨-Optimizer"><a href="#ä¼˜åŒ–å™¨-Optimizer" class="headerlink" title="ä¼˜åŒ–å™¨(Optimizer)"></a>ä¼˜åŒ–å™¨(Optimizer)</h3><p>PyTorch æä¾›äº†å¤šç§ä¼˜åŒ–å™¨ï¼ˆå¦‚ SGD, Adam ç­‰ï¼‰æ¥ä¼˜åŒ–ç¥ç»ç½‘ç»œçš„å‚æ•°.<br>ä¼˜åŒ–å™¨çš„ä½œç”¨æ˜¯æ›´æ–°ç¥ç»ç½‘ç»œçš„æƒé‡ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚ä¼˜åŒ–å™¨é€šè¿‡è®¡ç®—æ¢¯åº¦æ¥æ›´æ–°æƒé‡ï¼Œä»è€Œé€æ­¥é€¼è¿‘æœ€ä¼˜è§£ã€‚<br>æˆ‘ä»¬ä¸€èˆ¬å°±ç”¨Adamå’ŒSGDæ¯”è¾ƒå¤š</p>
<h4 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h4><ul>
<li>é€‚åˆå°å‹æˆ–ç®€å•ä»»åŠ¡</li>
<li>å¯ä»¥æ·»åŠ åŠ¨é‡ï¼ˆmomentumï¼‰ï¼Œæ”¹å–„ä¼˜åŒ–è·¯å¾„çš„å¹³æ»‘æ€§ï¼Œæœ‰åŠ©äºæ›´å¿«åœ°æ”¶æ•›ã€‚<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">å‚æ•°ï¼š</span></span><br><span class="line"><span class="string">lr: å­¦ä¹ ç‡ã€‚</span></span><br><span class="line"><span class="string">momentum: åŠ¨é‡å› å­ï¼Œç”¨äºåŠ é€Ÿæ”¶æ•›ï¼Œå‡å°‘éœ‡è¡ã€‚</span></span><br><span class="line"><span class="string">weight_decay: æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰ã€‚</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="è®­ç»ƒå¾ªç¯ï¼ˆTraining-Loopï¼‰"><a href="#è®­ç»ƒå¾ªç¯ï¼ˆTraining-Loopï¼‰" class="headerlink" title="è®­ç»ƒå¾ªç¯ï¼ˆTraining Loopï¼‰"></a>è®­ç»ƒå¾ªç¯ï¼ˆTraining Loopï¼‰</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å‡è®¾ä½ æœ‰è®­ç»ƒæ•°æ®</span></span><br><span class="line">x_train = torch.randn(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">y_train = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰æ¨¡å‹</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒå¾ªç¯</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># è®­ç»ƒ 100 ä¸ª epoch</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦</span></span><br><span class="line">    output = model(x_train)  <span class="comment"># å‰å‘ä¼ æ’­</span></span><br><span class="line">    loss = criterion(output, y_train)  <span class="comment"># è®¡ç®—æŸå¤±</span></span><br><span class="line">    loss.backward()  <span class="comment"># åå‘ä¼ æ’­</span></span><br><span class="line">    optimizer.step()  <span class="comment"># æ›´æ–°å‚æ•°</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/100], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">`</span><br></pre></td></tr></table></figure>

<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><ul>
<li>è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ï¼Œæ”¶æ•›é€Ÿåº¦å¿«ã€‚</li>
<li>å¯¹åˆå§‹å­¦ä¹ ç‡ä¸æ•æ„Ÿï¼Œé€‚åˆå¤æ‚æ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®é›†ã€‚<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">å‚æ•°ï¼š</span></span><br><span class="line"><span class="string">lr: å­¦ä¹ ç‡ã€‚</span></span><br><span class="line"><span class="string">betas: ä¸€å¯¹è¶…å‚æ•°ï¼Œæ§åˆ¶ä¸€é˜¶å’ŒäºŒé˜¶åŠ¨é‡çš„æŒ‡æ•°è¡°å‡ç‡ã€‚</span></span><br><span class="line"><span class="string">eps: é˜²æ­¢åˆ†æ¯ä¸ºé›¶çš„å°å¸¸æ•°ã€‚</span></span><br><span class="line"><span class="string">weight_decay: æƒé‡è¡°å‡ã€‚</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-8</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="æ•°æ®åŠ è½½-DataLoader"><a href="#æ•°æ®åŠ è½½-DataLoader" class="headerlink" title="æ•°æ®åŠ è½½(DataLoader)"></a>æ•°æ®åŠ è½½(DataLoader)</h3><p>PyTorch çš„ DataLoader ç”¨äºæ‰¹é‡åŠ è½½æ•°æ®ï¼Œé€šå¸¸ä¸ Dataset ç±»ç»“åˆä½¿ç”¨æ¥ç®¡ç†å’ŒåŠ è½½æ•°æ®é›†ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">a_dataloader</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data_path = data_path</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> os.path.exists(<span class="variable language_">self</span>.data_path)</span><br><span class="line"></span><br><span class="line">        df = pd.read_csv(<span class="variable language_">self</span>.data_path,name=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">        d=&#123;<span class="string">&quot;shirt&quot;</span>:<span class="number">0</span>,<span class="string">&quot;pants&quot;</span>:<span class="number">1</span>,<span class="string">&quot;shoes&quot;</span>:<span class="number">2</span>&#125;</span><br><span class="line">        df[<span class="number">4</span>]=df[<span class="number">4</span>].<span class="built_in">map</span>(d)  <span class="comment"># æŠŠæ¯æ¡æ•°æ®çš„ç¬¬äº”é¡¹ï¼ŒæŒ‰ç…§dæ›¿æ¢</span></span><br><span class="line"></span><br><span class="line">        data =df.iloc[:, :<span class="number">4</span>] <span class="comment">#å–æ‰€æœ‰è¡Œï¼Œ0-ç¬¬4åˆ— -&gt;ç‰¹å¾</span></span><br><span class="line">        label =df.iloc[:,<span class="number">4</span>:] <span class="comment">#æ ‡ç­¾</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#å½’ä¸€åŒ–æ•°æ®</span></span><br><span class="line">        data=(data-data.<span class="built_in">min</span>())/(data.<span class="built_in">max</span>()-data.<span class="built_in">min</span>())</span><br><span class="line"></span><br><span class="line">        <span class="comment">#æŠŠpdæ ¼å¼çš„æ•°æ®é›†ï¼Œè½¬æˆnpçš„arrayï¼Œå†è½¬æˆtorchçš„å¼ é‡</span></span><br><span class="line">        <span class="variable language_">self</span>.data=torch.from_numpy(np.array(data , dtype=<span class="string">&#x27;float32&#x27;</span>))</span><br><span class="line">        <span class="variable language_">self</span>.label=torch.from_numpy(np.array(label , dtype=<span class="string">&#x27;int64&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.data_num=<span class="built_in">len</span>(label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ç»§æ‰¿Datasetç±»ï¼Œå¿…é¡»é‡å†™è¿™ä¸¤ä¸ªæ–¹æ³•</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data_num</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data=<span class="built_in">list</span>(<span class="variable language_">self</span>.data)</span><br><span class="line">        <span class="variable language_">self</span>.label=<span class="built_in">list</span>(<span class="variable language_">self</span>.label)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[index], <span class="variable language_">self</span>.label[index]</span><br></pre></td></tr></table></figure>

<h3 id="è®­ç»ƒå’ŒéªŒè¯"><a href="#è®­ç»ƒå’ŒéªŒè¯" class="headerlink" title="è®­ç»ƒå’ŒéªŒè¯"></a>è®­ç»ƒå’ŒéªŒè¯</h3><h4 id="è®­ç»ƒ"><a href="#è®­ç»ƒ" class="headerlink" title="è®­ç»ƒ"></a>è®­ç»ƒ</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æ¨¡æ‹Ÿæ•°æ®</span></span><br><span class="line">x_train = torch.rand(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">y_train = torch.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># å‰å‘ä¼ æ’­</span></span><br><span class="line">    outputs = model(x_train)</span><br><span class="line">    loss = loss_fn(outputs, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åå‘ä¼ æ’­å’Œä¼˜åŒ–</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># æ¢¯åº¦æ¸…é›¶</span></span><br><span class="line">    loss.backward()        <span class="comment"># è®¡ç®—æ¢¯åº¦</span></span><br><span class="line">    optimizer.step()       <span class="comment"># æ›´æ–°å‚æ•°</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="éªŒè¯"><a href="#éªŒè¯" class="headerlink" title="éªŒè¯"></a>éªŒè¯</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># ç¦ç”¨æ¢¯åº¦è®¡ç®—</span></span><br><span class="line">    predictions = model(x_train)</span><br></pre></td></tr></table></figure>


<h3 id="æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½"><a href="#æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½" class="headerlink" title="æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½"></a>æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä¿å­˜æ¨¡å‹</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½æ¨¡å‹</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼</span></span><br></pre></td></tr></table></figure>

<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><p>PyTorch ä¸­çš„æ ¸å¿ƒæ¨¡å—ï¼Œæä¾›äº†å¤§é‡ç”¨äºæ„å»ºç¥ç»ç½‘ç»œçš„åŠŸèƒ½,ä»¥ä¸‹æ˜¯å¸¸ç”¨çš„ï¼š</p>
<ul>
<li><p>torch.nn.linearï¼šå…¨è¿æ¥å±‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layer = torch.nn.Linear(<span class="number">10</span>, <span class="number">5</span>)  <span class="comment"># è¾“å…¥ç»´åº¦10ï¼Œè¾“å‡ºç»´åº¦5</span></span><br></pre></td></tr></table></figure></li>
<li><p>torch.nn.Conv1d, Conv2d, Conv3d: ä¸€ç»´ã€äºŒç»´ã€ä¸‰ç»´å·ç§¯ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv = torch.nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>)  <span class="comment"># è¾“å…¥é€šé“3ï¼Œè¾“å‡ºé€šé“16ï¼Œå·ç§¯æ ¸3x3</span></span><br></pre></td></tr></table></figure></li>
<li><p>æ± åŒ–</p>
<blockquote>
<p>torch.nn.MaxPool1d, MaxPool2d, MaxPool3d: æœ€å¤§æ± åŒ–ã€‚<br>torch.nn.AvgPool1d, AvgPool2d, AvgPool3d: å¹³å‡æ± åŒ–ã€‚<br>torch.nn.AdaptiveAvgPool2d: è‡ªé€‚åº”å¹³å‡æ± åŒ–ï¼Œå¯å°†è¾“å‡ºè°ƒæ•´ä¸ºæŒ‡å®šå¤§å°ã€‚</p>
</blockquote>
</li>
<li><p>å½’ä¸€åŒ–</p>
<blockquote>
<p>torch.nn.BatchNorm1d, BatchNorm2d: æ‰¹é‡å½’ä¸€åŒ–ï¼Œå‡è½»åå˜é‡åç§»ã€‚<br>torch.nn.LayerNorm: å±‚å½’ä¸€åŒ–ã€‚<br>torch.nn.InstanceNorm2d: å®ä¾‹å½’ä¸€åŒ–ï¼Œå¸¸ç”¨äºé£æ ¼è¿ç§»ã€‚</p>
</blockquote>
</li>
<li><p>MultiheadAttention</p>
<blockquote>
<p>embed_dim: è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªè¯çš„åµŒå…¥ç»´åº¦ï¼ˆå³ç‰¹å¾ç»´åº¦ï¼‰ã€‚<br>num_heads: æ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚embed_dim å¿…é¡»å¯ä»¥è¢« num_heads æ•´é™¤ã€‚<br>dropout (é»˜è®¤ï¼š0.0): åœ¨æ³¨æ„åŠ›æƒé‡ä¸Šçš„ dropout æ¯”ä¾‹ã€‚<br>bias (é»˜è®¤ï¼šTrue): æ˜¯å¦åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­ä½¿ç”¨åç½®ã€‚<br>batch_first (é»˜è®¤ï¼šFalse): å¦‚æœä¸º Trueï¼Œè¾“å…¥çš„ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯ batch_sizeï¼Œå¦åˆ™æ˜¯åºåˆ—é•¿åº¦ã€‚</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åˆå§‹åŒ– MultiheadAttention æ¨¡å—</span></span><br><span class="line">embed_dim = <span class="number">16</span></span><br><span class="line">num_heads = <span class="number">4</span></span><br><span class="line">multihead_attn = MultiheadAttention(embed_dim, num_heads)</span><br><span class="line"><span class="comment"># è¾“å…¥æ•°æ®</span></span><br><span class="line">query = torch.rand(<span class="number">10</span>, <span class="number">32</span>, embed_dim)  <span class="comment"># (seq_len, batch_size, embed_dim)</span></span><br><span class="line">key = torch.rand(<span class="number">20</span>, <span class="number">32</span>, embed_dim)</span><br><span class="line">value = torch.rand(<span class="number">20</span>, <span class="number">32</span>, embed_dim)</span><br><span class="line"><span class="comment"># å‰å‘ä¼ æ’­</span></span><br><span class="line">output, attn_weights = multihead_attn(query, key, value)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># (10, 32, 16)</span></span><br><span class="line"><span class="built_in">print</span>(attn_weights.shape)  <span class="comment"># (32 * 4, 10, 20)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>torch.nn.EmbeddingåµŒå…¥å±‚</p>
<blockquote>
<p>num_embeddings: åµŒå…¥çŸ©é˜µçš„è¯è¡¨å¤§å°ï¼Œå³æ€»å…±å¯ä»¥åµŒå…¥å¤šå°‘ä¸ªç´¢å¼•ã€‚<br>embedding_dim: æ¯ä¸ªåµŒå…¥å‘é‡çš„ç»´åº¦ã€‚<br>padding_idx (é»˜è®¤ï¼šNone): å¦‚æœæŒ‡å®šï¼Œè¯¥ç´¢å¼•çš„åµŒå…¥å‘é‡ä¼šåˆå§‹åŒ–ä¸ºé›¶ï¼Œä¸”ä¸ä¼šè¢«æ¢¯åº¦æ›´æ–°ã€‚<br>max_norm (é»˜è®¤ï¼šNone): å¦‚æœæŒ‡å®šï¼ŒåµŒå…¥å‘é‡çš„ L2 èŒƒæ•°ä¼šè¢«çº¦æŸåˆ°è¯¥å€¼ã€‚<br>scale_grad_by_freq (é»˜è®¤ï¼šFalse): æ˜¯å¦æ ¹æ®ç´¢å¼•é¢‘ç‡å¯¹æ¢¯åº¦è¿›è¡Œç¼©æ”¾ã€‚</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åˆ›å»º Embedding å±‚</span></span><br><span class="line">vocab_size = <span class="number">10</span>  <span class="comment"># è¯è¡¨å¤§å°</span></span><br><span class="line">embedding_dim = <span class="number">5</span>  <span class="comment"># åµŒå…¥ç»´åº¦</span></span><br><span class="line">embedding = Embedding(vocab_size, embedding_dim)</span><br><span class="line"><span class="comment"># è¾“å…¥æ•°æ®ï¼ˆç´¢å¼•åºåˆ—ï¼‰</span></span><br><span class="line">input_indices = torch.tensor([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>])</span><br><span class="line">output = embedding(input_indices)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># (4, 5)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>torch.nn.Parameter</p>
<blockquote>
<p>ä¸€ç§ç‰¹æ®Šçš„å¼ é‡ï¼Œç”¨äºè¡¨ç¤ºæ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°ï¼ˆå³åœ¨è®­ç»ƒä¸­éœ€è¦ä¼˜åŒ–çš„æƒé‡ï¼‰<br>torch.nn.Parameter æ˜¯ torch.Tensor çš„å­ç±»ã€‚<br>ä¸æ™®é€šå¼ é‡ä¸åŒï¼Œå½“ Parameter è¢«èµ‹ç»™ torch.nn.Module çš„å±æ€§æ—¶ï¼Œä¼šè‡ªåŠ¨æ·»åŠ åˆ°æ¨¡å‹çš„å‚æ•°åˆ—è¡¨ä¸­ï¼Œå‚ä¸æ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–ã€‚</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># æ·»åŠ ä¸€ä¸ªå¯å­¦ä¹ å‚æ•°</span></span><br><span class="line">        <span class="variable language_">self</span>.weight = Parameter(torch.randn(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.matmul(x, <span class="variable language_">self</span>.weight)</span><br><span class="line">model = MyModel()</span><br><span class="line">input_data = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = model(input_data)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># (3, 5)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>æ¿€æ´»å‡½æ•°</p>
<ul>
<li>torch.nn.ReLU</li>
<li>torch.nn.Sigmoid</li>
<li>torch.nn.Tanh</li>
<li>ç­‰ç­‰</li>
</ul>
</li>
<li><p>æŸå¤±å‡½æ•°</p>
<ul>
<li>å›å½’æŸå¤±<blockquote>
<p>torch.nn.MSELoss: å‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼‰ã€‚<br>torch.nn.L1Loss: å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean Absolute Errorï¼‰ã€‚<br>torch.nn.SmoothL1Loss: å¹³æ»‘çš„ L1 æŸå¤±ã€‚</p>
</blockquote>
</li>
<li>åˆ†ç±»æŸå¤±<blockquote>
<p>torch.nn.CrossEntropyLoss: äº¤å‰ç†µæŸå¤±ï¼Œé€‚ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ã€‚<br>torch.nn.BCELoss, BCEWithLogitsLoss: äºŒå…ƒäº¤å‰ç†µæŸå¤±ã€‚<br>torch.nn.NLLLoss: è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œé€šå¸¸å’Œ LogSoftmax ä¸€èµ·ç”¨ã€‚</p>
</blockquote>
</li>
</ul>
</li>
<li><p>torch.nn.Dropout<br>ä¸¢å¼ƒå±‚</p>
</li>
</ul>
<h2 id="ç¤ºä¾‹"><a href="#ç¤ºä¾‹" class="headerlink" title="ç¤ºä¾‹"></a>ç¤ºä¾‹</h2><h3 id="ä¸€ä¸ªMLPçš„å®ç°"><a href="#ä¸€ä¸ªMLPçš„å®ç°" class="headerlink" title="ä¸€ä¸ªMLPçš„å®ç°"></a>ä¸€ä¸ªMLPçš„å®ç°</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> a_dataloader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#åˆå§‹åŒ–ç¥ç»ç½‘ç»œ</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="comment">#å‚æ•°ï¼šè¾“å‡ºå±‚ï¼Œéšè—å±‚ï¼Œè¾“å‡ºå±‚ç»“ç‚¹æ•°é‡(ç»´åº¦)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_dim,hidden1_dim,hidden2_dim,hidden3_dim,out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>.__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layer1=nn.Linear(in_dim,hidden1_dim)  <span class="comment"># è¾“å…¥å±‚</span></span><br><span class="line">        <span class="variable language_">self</span>.layer2=nn.Linear(hidden1_dim,hidden2_dim)</span><br><span class="line">        <span class="variable language_">self</span>.layer3=nn.Linear(hidden2_dim,hidden3_dim)</span><br><span class="line">        <span class="variable language_">self</span>.layer4=nn.Linear(hidden3_dim,out_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=<span class="variable language_">self</span>.layer1(x)</span><br><span class="line">        x=<span class="variable language_">self</span>.layer2(x)</span><br><span class="line">        x=<span class="variable language_">self</span>.layer3(x)</span><br><span class="line">        x=<span class="variable language_">self</span>.layer4(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰è®¡ç®—ç¯å¢ƒ</span></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†</span></span><br><span class="line">custom_dataset=a_dataloader(<span class="string">&quot;.&quot;</span>)  <span class="comment">#è¿™é‡Œå†™æ•°æ®é›†çš„url</span></span><br><span class="line"><span class="comment"># 70%çš„æ•°æ®é›†åšè®­ç»ƒï¼Œ20%åšéªŒè¯ï¼Œ10%åšæµ‹è¯•</span></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(custom_dataset)*<span class="number">0.7</span>)</span><br><span class="line">valid_size = <span class="built_in">int</span>(<span class="built_in">len</span>(custom_dataset)*<span class="number">0.2</span>)</span><br><span class="line">test_size = <span class="built_in">len</span>(custom_dataset) - train_size - valid_size</span><br><span class="line"><span class="comment"># åˆ†å‰²æ•°æ®é›†ï¼štorch.utils.data.random_split</span></span><br><span class="line">train_dataset, valid_dataset, test_dataset = (</span><br><span class="line">    torch.utils.data.random_split(custom_dataset, [train_size, valid_size, test_size]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰æ•°æ®åŠ è½½å™¨</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)  <span class="comment"># shuffle: æ¯æ¬¡éƒ½éšæœºæ‰“ä¹±æ•°æ®</span></span><br><span class="line">valid_loader = DataLoader(valid_dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_loader)*<span class="number">64</span> , <span class="built_in">len</span>(valid_loader) , <span class="built_in">len</span>(test_loader))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨ç†å‡½æ•°ï¼Œè¿”å›é¢„æµ‹å‡†ç¡®ç‡</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">infer</span>(<span class="params">model,dataset,device</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># è¿›å…¥éªŒè¯æ¨¡å¼</span></span><br><span class="line">    accuracy = <span class="number">0</span>  <span class="comment">#é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°é‡</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():<span class="comment"># ä¸è®¡ç®—æ¢¯åº¦: åŠ é€Ÿæ¨ç†</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">            datas, labels = data</span><br><span class="line">            outputs=model(datas.to(device))  <span class="comment">#è¾“å‡ºæ˜¯æ¯ä¸ªç±»å‹çš„å¯èƒ½æ€§</span></span><br><span class="line">            predict_y=torch.<span class="built_in">max</span>(outputs,dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># å¯èƒ½æ€§æœ€å¤§çš„ç±»åˆ«ï¼Œå°±æ˜¯é¢„æµ‹ç»“æœ</span></span><br><span class="line">            accuracy+=torch.eq(predict_y,labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line">            <span class="comment">#eq: æ¯”è¾ƒä¸¤ä¸ªå¼ é‡ä¸­å¯¹åº”ä½ç½®çš„å…ƒç´ æ˜¯å¦ç›¸ç­‰ã€‚</span></span><br><span class="line">            <span class="comment">#.sum()ï¼šå¼ é‡ä¸­çš„æ‰€æœ‰å…ƒç´ è¿›è¡Œæ±‚å’Œã€‚ç”±äºTrue=1ï¼ŒFalse=0ï¼Œæ‰€ä»¥è¿™ä¸ªæ±‚å’Œæ“ä½œå®é™…ä¸Šè®¡ç®—äº†æ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°é‡ã€‚</span></span><br><span class="line">            <span class="comment">#.item():sumå¾—å‡ºæ¥çš„æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œç”¨.item()æŠŠå®ƒå˜æˆä¸€ä¸ªæ•°å­—</span></span><br><span class="line">    acc_res=accuracy/<span class="built_in">len</span>(dataset)</span><br><span class="line">    <span class="keyword">return</span> acc_res</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">lr=<span class="number">0.005</span>,epochs=<span class="number">10</span></span>):</span><br><span class="line">    model=Net(<span class="number">4</span>,<span class="number">100</span>,<span class="number">50</span>,<span class="number">20</span>,<span class="number">3</span>).to(device)</span><br><span class="line">    <span class="comment"># å®šä¹‰æŸå¤±å‡½æ•°</span></span><br><span class="line">    loss_func = nn.CrossEntropyLoss() <span class="comment"># äº¤å‰ç†µæŸå¤±</span></span><br><span class="line">    pg=[p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad] <span class="comment">#åªæœ‰æ»¡è¶³æ¡ä»¶çš„å‚æ•°(å³requires_gradä¸ºTrue)æ‰ä¼šè¢«æ·»åŠ åˆ°åˆ—è¡¨ pg ä¸­ã€‚</span></span><br><span class="line">    <span class="comment">#ä¼˜åŒ–å™¨</span></span><br><span class="line">    optimizer=optim.Adam(pg,lr=lr)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#æƒé‡æ–‡ä»¶å­˜å‚¨è·¯å¾„</span></span><br><span class="line">    save_path=os.path.join(os.getcwd(),<span class="string">&quot;ç›¸å¯¹é¡¹ç›®çš„url&quot;</span>)<span class="comment">#os.getcwd()è·å¾—é¡¹ç›®è·¯å¾„</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(save_path) <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">        os.mkdir(save_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#å¼€å§‹è®­ç»ƒ</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        accuracy = torch.zeros(<span class="number">1</span>).to(device)</span><br><span class="line">        sample_num =<span class="number">0</span></span><br><span class="line">        train_bar=tqdm(train_loader,file=sys.stdout,ncaols=<span class="number">100</span>) <span class="comment">#è¿›åº¦æ¡</span></span><br><span class="line">        <span class="keyword">for</span> datas <span class="keyword">in</span> train_bar:</span><br><span class="line">            data,label=datas</span><br><span class="line">            label=label.squeeze(-<span class="number">1</span>) <span class="comment"># å»æ‰labelçš„æœ€åä¸€ä¸ªç»´åº¦</span></span><br><span class="line">            sample_num+=data.shape[<span class="number">0</span>] <span class="comment"># æ ·æœ¬æ•°é‡</span></span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad() <span class="comment"># æ¢¯åº¦æ¸…é›¶</span></span><br><span class="line">            data.to(device)</span><br><span class="line">            outputs=model(data.to(device))</span><br><span class="line">            pred_class=torch.<span class="built_in">max</span>(outputs,dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># å¯èƒ½æ€§æœ€å¤§çš„ç±»åˆ«ï¼Œå°±æ˜¯é¢„æµ‹ç»“æœ,å› ä¸ºtorch.maxè¿”å›ä¸€ä¸ªå…ƒç»„</span></span><br><span class="line">            <span class="comment">#ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯maxçš„å€¼ï¼Œç¬¬äºŒä¸ªæ˜¯maxçš„ç´¢å¼•ï¼Œæˆ‘ä»¬éœ€è¦ç´¢å¼•</span></span><br><span class="line">            accuracy=torch.eq(pred_class,label.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line">            loss=loss_func(outputs,label.to(device))</span><br><span class="line">            loss.backward() <span class="comment"># åå‘ä¼ æ’­</span></span><br><span class="line">            optimizer.step() <span class="comment"># æ›´æ–°å‚æ•°</span></span><br><span class="line"></span><br><span class="line">            train_acc=accuracy/sample_num</span><br><span class="line">            train_bar.desc=<span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125; acc:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>,epochs,loss.item(),train_acc)</span><br><span class="line">            <span class="comment">#è®¾ç½®è¿›åº¦æ¡çš„å±æ€§</span></span><br><span class="line">        <span class="comment"># éªŒè¯é›†</span></span><br><span class="line">        val_acc=infer(model,valid_loader,device)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125; acc:&#123;:.3f&#125; val_acc:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>,epochs,loss.item(),val_acc,val_acc))</span><br><span class="line">        torch.save(model.state_dict(),save_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#æ¯æ¬¡æ•°æ®è¿­ä»£ä¹‹åï¼ŒæŠŠåˆå§‹åŒ–çš„æŒ‡æ ‡æ¸…é›¶</span></span><br><span class="line">        train_acc=<span class="number">0</span></span><br><span class="line">        val_acc=<span class="number">0</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;training finished&quot;</span>)</span><br><span class="line">    test_acc=infer(model,test_loader,device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;test_acc: &quot;</span>,test_acc)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h3 id="ViTæ¨¡å‹-è®­ç»ƒ"><a href="#ViTæ¨¡å‹-è®­ç»ƒ" class="headerlink" title="ViTæ¨¡å‹+è®­ç»ƒ"></a>ViTæ¨¡å‹+è®­ç»ƒ</h3><h4 id="æ¨¡å‹éƒ¨åˆ†"><a href="#æ¨¡å‹éƒ¨åˆ†" class="headerlink" title="æ¨¡å‹éƒ¨åˆ†"></a>æ¨¡å‹éƒ¨åˆ†</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"></span><br><span class="line"><span class="comment"># helpers</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pair</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="comment"># å¦‚æœè¾“å…¥æ˜¯å…ƒç»„ï¼Œç›´æ¥è¿”å›ï¼›å¦åˆ™å°†è¾“å…¥é‡å¤ä¸¤æ¬¡ç»„æˆå…ƒç»„ã€‚</span></span><br><span class="line">    <span class="keyword">return</span> t <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">else</span> (t, t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># classes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    å‰å‘å…¨è¿æ¥ç½‘ç»œï¼ˆFeedForwardï¼‰ï¼Œç”¨äº Transformer çš„ MLP å±‚ã€‚</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># Sequenstial:å®¹å™¨æ¨¡å—ï¼Œç”¨äºå°†ä¸€ç³»åˆ—å­æ¨¡å—æŒ‰é¡ºåºç»„åˆåœ¨ä¸€èµ·ã€‚è¿™äº›å­æ¨¡å—ä¼šæŒ‰ç…§å®šä¹‰çš„é¡ºåºä¾æ¬¡æ‰§è¡Œ,å¯ä»¥ä»£æ›¿å®šä¹‰forwardå‡½æ•°</span></span><br><span class="line">        <span class="variable language_">self</span>.net = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),        <span class="comment"># å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ã€‚</span></span><br><span class="line">            nn.Linear(dim, hidden_dim), <span class="comment"># ç¬¬ä¸€å±‚å…¨è¿æ¥ï¼Œå°†ç»´åº¦ä» dim æ‰©å±•åˆ° hidden_dimã€‚</span></span><br><span class="line">            nn.GELU(),                <span class="comment"># ä½¿ç”¨ GELU æ¿€æ´»å‡½æ•°ã€‚</span></span><br><span class="line">            nn.Dropout(dropout),      <span class="comment"># é˜²æ­¢è¿‡æ‹Ÿåˆçš„ Dropoutã€‚</span></span><br><span class="line">            nn.Linear(hidden_dim, dim), <span class="comment"># ç¬¬äºŒå±‚å…¨è¿æ¥ï¼Œå°†ç»´åº¦ä» hidden_dim ç¼©å‡å› dimã€‚</span></span><br><span class="line">            nn.Dropout(dropout)       <span class="comment"># å†æ¬¡ä½¿ç”¨ Dropoutã€‚</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.net(x)  <span class="comment"># æ‰§è¡Œå‰å‘ä¼ æ’­ã€‚</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Self-Attention)ã€‚</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads = <span class="number">8</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head *  heads  <span class="comment"># å†…éƒ¨æ€»ç»´åº¦ä¸º dim_head * headsã€‚</span></span><br><span class="line">        <span class="comment">#æ¯ä¸ªå¤´çš„è¾“å‡ºä¼šè¢«æ‹¼æ¥æˆä¸€ä¸ªæ›´å¤§çš„å¼ é‡ã€‚éœ€è¦é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆprojection layerï¼‰å°†æ‹¼æ¥åçš„å¼ é‡æŠ•å½±å›åŸå§‹çš„è¾“å…¥ç»´åº¦ï¼Œä»¥ç¡®ä¿è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦åŒ¹é…</span></span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim)  <span class="comment"># å¦‚æœåªæœ‰ä¸€ä¸ªæ³¨æ„åŠ›å¤´ï¼Œå¹¶ä¸”å®ƒçš„ç»´åº¦å’Œè¾“å…¥çš„ç»´åº¦ä¸€è‡´ï¼Œå°±ä¸éœ€è¦çº¿æ€§å˜æ¢æ¥æŠ•å½±</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.heads = heads  <span class="comment"># å¤šå¤´çš„æ•°é‡ã€‚</span></span><br><span class="line">        <span class="variable language_">self</span>.scale = dim_head ** -<span class="number">0.5</span>  <span class="comment"># ç¼©æ”¾å› å­ï¼Œç”¨äºç¨³å®šæ¢¯åº¦ã€‚(å°±æ˜¯è‡ªæ³¨æ„åŠ›å…¬å¼åˆ†æ¯ä¸Šçš„æ ¹å·ä¸‹dk)</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.norm = nn.LayerNorm(dim)  <span class="comment"># è¾“å…¥å½’ä¸€åŒ–ã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.attend = nn.Softmax(dim = -<span class="number">1</span>)  <span class="comment"># ç”¨ Softmax è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># Dropout é˜²æ­¢è¿‡æ‹Ÿåˆã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias = <span class="literal">False</span>)  <span class="comment"># å°†è¾“å…¥æŠ•å½±åˆ° query, key, valueã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># è¾“å‡ºæŠ•å½±å±‚ï¼Œå¦‚æœéœ€è¦åˆ™æ·»åŠ ï¼Œå¦åˆ™ç›´æ¥ä¼ é€’ã€‚</span></span><br><span class="line">        <span class="variable language_">self</span>.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity() <span class="comment"># æŠ•å½±ã€‚</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)  <span class="comment"># å¯¹è¾“å…¥å½’ä¸€åŒ–ã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ç”Ÿæˆ q, k, v çš„å¼ é‡ï¼Œåˆ†åˆ«è¡¨ç¤º queryã€key å’Œ valueã€‚</span></span><br><span class="line">        <span class="comment"># chunkï¼šå°†å¼ é‡åˆ†å‰²æˆæŒ‡å®šæ•°é‡çš„å—ã€‚ä¹‹å‰qkvæ˜¯è¿èµ·æ¥çš„ï¼Œç°åœ¨æŒ‰ç…§æœ€åä¸€ä¸ªç»´åº¦(dim=-1)ï¼Œå³ç‰¹å¾ç»´åº¦åˆ†å‰²æˆä¸‰éƒ¨åˆ†</span></span><br><span class="line">        <span class="comment"># åŸæ¥æ˜¯bï¼Œnï¼Œ3*h*dï¼Œç°åœ¨åˆ†æˆbï¼Œnï¼Œh*dï¼Œ</span></span><br><span class="line">        <span class="comment"># chunkè¿”å›çš„æ˜¯ä¸€ä¸ª**å…ƒç»„**</span></span><br><span class="line">        qkv = <span class="variable language_">self</span>.to_qkv(x).chunk(<span class="number">3</span>, dim = -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># mapï¼šæŠŠåŒä¸€ä¸ªå‡½æ•°åº”ç”¨äºæ¯ä¸ªå…ƒç´ ã€‚</span></span><br><span class="line">        <span class="comment"># rearrangeï¼šé‡å¡‘å¼ é‡çš„ç»´åº¦æˆèƒ½è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°çš„æ ¼å¼</span></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h = <span class="variable language_">self</span>.heads), qkv)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ã€‚</span></span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * <span class="variable language_">self</span>.scale</span><br><span class="line"></span><br><span class="line">        attn = <span class="variable language_">self</span>.attend(dots)  <span class="comment"># å¯¹æ³¨æ„åŠ›åˆ†æ•°ä½¿ç”¨ Softmaxã€‚</span></span><br><span class="line">        attn = <span class="variable language_">self</span>.dropout(attn)  <span class="comment"># Dropout é˜²æ­¢è¿‡æ‹Ÿåˆã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># è®¡ç®—æ³¨æ„åŠ›æƒé‡ä¸ value çš„åŠ æƒå’Œã€‚</span></span><br><span class="line">        out = torch.matmul(attn, v)</span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)  <span class="comment"># é‡å¡‘å¼ é‡ä»¥è¿˜åŸåˆ°è¾“å…¥çš„å½¢çŠ¶ã€‚</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.to_out(out)  <span class="comment"># è¾“å‡ºæŠ•å½±ã€‚</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Transformer ç¼–ç å™¨ï¼Œç”±å¤šä¸ªæ³¨æ„åŠ›å±‚å’Œå‰å‘å…¨è¿æ¥å±‚ç»„æˆã€‚</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.norm = nn.LayerNorm(dim)  <span class="comment"># æœ€åè¾“å‡ºçš„å½’ä¸€åŒ–å±‚ã€‚</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList([])  <span class="comment"># å­˜å‚¨æ‰€æœ‰ Transformer å±‚ã€‚</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            <span class="comment"># æ¯ä¸ª Transformer å±‚ç”±ä¸€ä¸ª Attention å’Œä¸€ä¸ª FeedForward ç»„æˆã€‚</span></span><br><span class="line">            <span class="variable language_">self</span>.layers.append(nn.ModuleList([</span><br><span class="line">                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),</span><br><span class="line">                FeedForward(dim, mlp_dim, dropout = dropout)</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            <span class="comment"># è‡ªæ³¨æ„åŠ›å±‚ï¼ŒåŠ å…¥æ®‹å·®è¿æ¥ã€‚</span></span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            <span class="comment"># å‰å‘å…¨è¿æ¥å±‚ï¼ŒåŠ å…¥æ®‹å·®è¿æ¥ã€‚</span></span><br><span class="line">            x = ff(x) + x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(x)  <span class="comment"># æœ€ç»ˆå½’ä¸€åŒ–è¾“å‡ºã€‚</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Vision Transformer æ¨¡å‹ã€‚</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    å‚æ•°ï¼š</span></span><br><span class="line"><span class="string">    num_classesï¼šåˆ†ç±»ä»»åŠ¡ä¸­çš„ç±»åˆ«æ•°ã€‚</span></span><br><span class="line"><span class="string">    dimï¼šTransformer è¾“å…¥ç‰¹å¾çš„ç»´åº¦ã€‚</span></span><br><span class="line"><span class="string">    depthï¼šTransformer ç¼–ç å™¨çš„å±‚æ•°ã€‚</span></span><br><span class="line"><span class="string">    headsï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å¤´æ•°ã€‚</span></span><br><span class="line"><span class="string">    mlp_dimï¼šMLP å±‚çš„ç»´åº¦ã€‚</span></span><br><span class="line"><span class="string">    poolï¼šè¡¨ç¤ºç”¨äºåˆ†ç±»çš„æ± åŒ–æ–¹å¼ï¼Œå¯ä»¥æ˜¯ &#x27;cls&#x27;ï¼ˆå– [CLS] tokenï¼‰æˆ– &#x27;mean&#x27;ï¼ˆå¯¹æ‰€æœ‰ token æ±‚å¹³å‡ï¼‰ã€‚</span></span><br><span class="line"><span class="string">    channelsï¼šè¾“å…¥å›¾åƒçš„é€šé“æ•°ï¼ˆä¾‹å¦‚ï¼ŒRGB å›¾åƒä¸º 3ï¼‰ã€‚</span></span><br><span class="line"><span class="string">    dim_headï¼šæ¯ä¸ªè‡ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ã€‚</span></span><br><span class="line"><span class="string">    dropout å’Œ emb_dropoutï¼šç”¨äºåœ¨è®­ç»ƒæœŸé—´å‡å°‘è¿‡æ‹Ÿåˆçš„ Dropout æ¦‚ç‡ã€‚</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span>, emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        image_height, image_width = pair(image_size)  <span class="comment"># å›¾åƒçš„é«˜åº¦å’Œå®½åº¦ã€‚</span></span><br><span class="line">        patch_height, patch_width = pair(patch_size)  <span class="comment"># æ¯ä¸ª patch çš„é«˜åº¦å’Œå®½åº¦ã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ç¡®ä¿å›¾åƒå°ºå¯¸å¯ä»¥è¢« patch å°ºå¯¸æ•´é™¤ã€‚</span></span><br><span class="line">        <span class="keyword">assert</span> image_height % patch_height == <span class="number">0</span> <span class="keyword">and</span> image_width % patch_width == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span></span><br><span class="line"></span><br><span class="line">        num_patches = (image_height // patch_height) * (image_width // patch_width)  <span class="comment"># patch æ€»æ•°é‡ã€‚</span></span><br><span class="line">        patch_dim = channels * patch_height * patch_width  <span class="comment"># æ¯ä¸ª patch å±•å¹³åçš„ç»´åº¦ã€‚</span></span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># å›¾åƒåˆ‡åˆ†ä¸º patchï¼Œå¹¶åµŒå…¥åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ã€‚</span></span><br><span class="line">        <span class="comment"># è¾“å…¥çš„å½¢çŠ¶ä¸º(batch_size, channels, height, width)ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º(batch_size, num_patches, patch_dim)</span></span><br><span class="line">        <span class="comment"># patch_dim æ˜¯æ¯ä¸ª patch å±•å¹³åçš„ç»´åº¦ã€‚</span></span><br><span class="line">        <span class="variable language_">self</span>.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_height, p2 = patch_width),</span><br><span class="line">            nn.LayerNorm(patch_dim),  <span class="comment"># å¯¹ patch çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ã€‚</span></span><br><span class="line">            nn.Linear(patch_dim, dim),  <span class="comment"># å°†åµŒå…¥ç»´åº¦è½¬æ¢ä¸º Transformer çš„è¾“å…¥ç»´åº¦ã€‚</span></span><br><span class="line">            nn.LayerNorm(dim),  <span class="comment"># å†æ¬¡å½’ä¸€åŒ–ã€‚</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))  <span class="comment"># ä½ç½®ç¼–ç å‚æ•°ã€‚</span></span><br><span class="line">        <span class="variable language_">self</span>.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))  <span class="comment"># åˆ†ç±» tokenã€‚</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(emb_dropout)  <span class="comment"># Dropoutã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)  <span class="comment"># Transformer ç¼–ç å™¨ã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.pool = pool  <span class="comment"># åˆ†ç±»æ–¹å¼ï¼š&#x27;cls&#x27; æˆ– &#x27;mean&#x27;ã€‚</span></span><br><span class="line">        <span class="variable language_">self</span>.to_latent = nn.Identity()  <span class="comment"># ä¼ é€’åˆ°æœ€ç»ˆåˆ†ç±»å¤´çš„å±‚ã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.mlp_head = nn.Linear(dim, num_classes)  <span class="comment"># æœ€ååˆ†ç±»å¤´ã€‚</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.to_patch_embedding(img)  <span class="comment"># å°†å›¾åƒè½¬æ¢ä¸º patch åµŒå…¥ã€‚</span></span><br><span class="line">        b, n, _ = x.shape</span><br><span class="line"></span><br><span class="line">        cls_tokens = repeat(<span class="variable language_">self</span>.cls_token, <span class="string">&#x27;1 1 d -&gt; b 1 d&#x27;</span>, b = b)  <span class="comment">#å¤åˆ¶åˆ†ç±» token ä»¥åŒ¹é…æ‰¹é‡å¤§å° b</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)  <span class="comment"># å°†åˆ†ç±» token æ·»åŠ åˆ° patch åºåˆ—ä¸­ã€‚</span></span><br><span class="line">        x += <span class="variable language_">self</span>.pos_embedding[:, :(n + <span class="number">1</span>)]  <span class="comment"># å°†ä½ç½®ç¼–ç æ·»åŠ åˆ°æ¯ä¸ª patch çš„åµŒå…¥å‘é‡ä¸­ã€‚</span></span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)  <span class="comment"># Dropoutã€‚</span></span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.transformer(x)  <span class="comment"># è¾“å…¥åˆ° Transformer ç¼–ç å™¨ã€‚</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># åˆ†ç±»æ–¹å¼ï¼šå– CLS token æˆ–è€…å¯¹æ‰€æœ‰ token å¹³å‡ã€‚</span></span><br><span class="line">        x = x.mean(dim = <span class="number">1</span>) <span class="keyword">if</span> <span class="variable language_">self</span>.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.to_latent(x)  <span class="comment"># ä¼ é€’åˆ°åˆ†ç±»å¤´å‰çš„å±‚ã€‚</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.mlp_head(x)  <span class="comment"># è¾“å‡ºåˆ†ç±»ç»“æœã€‚</span></span><br></pre></td></tr></table></figure>

<h4 id="è®­ç»ƒéƒ¨åˆ†"><a href="#è®­ç»ƒéƒ¨åˆ†" class="headerlink" title="è®­ç»ƒéƒ¨åˆ†"></a>è®­ç»ƒéƒ¨åˆ†</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms.functional <span class="keyword">import</span> to_tensor</span><br><span class="line"><span class="keyword">from</span> vit <span class="keyword">import</span> ViT</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ•°æ®é¢„å¤„ç†ï¼šè°ƒæ•´å¤§å°ã€æ‰©å±•é€šé“</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># ViT éœ€è¦å›ºå®šå¤§å°è¾“å…¥</span></span><br><span class="line">    transforms.Lambda(<span class="keyword">lambda</span> x: to_tensor(x).repeat(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)),  <span class="comment"># å°†å•é€šé“æ‰©å±•ä¸º 3 é€šé“</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># å½’ä¸€åŒ–</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½ MNIST æ•°æ®é›†</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;E:/CODE/python/D2L/data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">False</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;E:/CODE/python/D2L/data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»ºæ¨¡å‹å®ä¾‹</span></span><br><span class="line">model = ViT(</span><br><span class="line">    image_size=<span class="number">224</span>,</span><br><span class="line">    patch_size=<span class="number">16</span>,</span><br><span class="line">    num_classes=<span class="number">10</span>,  <span class="comment"># MNIST æ˜¯ 10 ç±»åˆ†ç±»é—®é¢˜</span></span><br><span class="line">    dim=<span class="number">64</span>,</span><br><span class="line">    depth=<span class="number">3</span>,</span><br><span class="line">    heads=<span class="number">3</span>,</span><br><span class="line">    mlp_dim=<span class="number">256</span>,</span><br><span class="line">    dropout=<span class="number">0.1</span>,</span><br><span class="line">    emb_dropout=<span class="number">0.1</span></span><br><span class="line">).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">3e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆå§‹åŒ– TensorBoard SummaryWriter</span></span><br><span class="line">writer = SummaryWriter(log_dir=<span class="string">&#x27;runs/vit_experiment&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒå’ŒéªŒè¯å¾ªç¯</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, train_loader, criterion, optimizer, epoch</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> tqdm(train_loader, desc=<span class="string">f&quot;Training Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>&quot;</span>):</span><br><span class="line">        imgs, labels = imgs.to(<span class="string">&#x27;cuda&#x27;</span>), labels.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># å‰å‘ä¼ æ’­</span></span><br><span class="line">        outputs = model(imgs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># åå‘ä¼ æ’­ä¸ä¼˜åŒ–</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    avg_loss = total_loss / <span class="built_in">len</span>(train_loader)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;avg_loss:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Training Loss&#x27;</span>, avg_loss, epoch)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, test_loader</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> tqdm(test_loader, desc=<span class="string">f&quot;Evaluating Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>&quot;</span>):</span><br><span class="line">            imgs, labels = imgs.to(<span class="string">&#x27;cuda&#x27;</span>), labels.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">            outputs = model(imgs)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    accuracy = correct / total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Validation Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>%&#125;</span>&quot;</span>)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Validation Accuracy&#x27;</span>, accuracy, epoch)</span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒå’ŒéªŒè¯</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    train(model, train_loader, criterion, optimizer, epoch)</span><br><span class="line">    evaluate(model, test_loader)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># æ·±åº¦å­¦ä¹ </a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/12/01/graph2/" rel="prev" title="å›¾ç®—æ³•æ¿å­2.0">
                  <i class="fa fa-angle-left"></i> å›¾ç®—æ³•æ¿å­2.0
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/12/01/tensorboard/" rel="next" title="PyTorchçŠ¶æ€ç›‘ç£">
                  PyTorchçŠ¶æ€ç›‘ç£ <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">PIGMilk</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"http://example.com/2024/12/01/pytorch2/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
